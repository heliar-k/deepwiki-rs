# 系统边界接口文档

本文档描述系统的外部调用接口，包括 CLI 命令、API 端点、配置参数和其他边界机制。

## 命令行接口 (CLI)

### litho

**描述**: 文档生成工具的主入口点。使用 AI 智能体分析源代码并生成 C4 架构文档。

**源文件**: `src/main.rs`

**参数**：

- `config` (PathBuf): 可选 - 配置文件 TOML 的路径

**使用示例**：

```bash
litho
```

```bash
litho --config ./litho.toml
```

```bash
litho --config /path/to/custom-config.toml
```

### litho sync-knowledge

**描述**: 从本地文档文件（Markdown、PDF、SQL 等）同步外部知识源到知识库，用于 RAG 增强的文档生成。检查缓存状态并对文档执行分块摄取。

**源文件**: `src/main.rs`

**参数**：

- `config` (PathBuf): 可选 - 配置文件 TOML 的路径（如果指定，覆盖顶级配置）
- `force` (bool): 可选 - 即使知识缓存是最新的也强制同步（默认：`false`）

**选项**：

- `force`(标志): 可选 - 绕过缓存验证强制同步（默认：`false`）
- `config`(PathBuf): 可选 - 配置文件路径

**使用示例**：

```bash
litho sync-knowledge
```

```bash
litho sync-knowledge --force
```

```bash
litho sync-knowledge --config ./litho.toml --force
```

## 集成建议

### 配置文件设置

使用配置文件进行基本项目集成。该工具期望在项目根目录中有一个 litho.toml 文件，或通过 --config 接受显式路径。配置包括 LLM 提供程序设置、项目元数据、排除模式和知识库集成。

**示例代码**：

```toml
# litho.toml
project_name = "my-awesome-project"
project_path = "."
output_path = "./docs/architecture"
target_language = "en"

[llm]
provider = "openai"
api_key = "${LITHO_LLM_API_KEY}"  # 来自环境变量
api_base_url = "https://api.openai.com/v1"
model_efficient = "gpt-4o-mini"
model_powerful = "gpt-4o"
max_tokens = 4096
temperature = 0.1
retry_attempts = 3

[cache]
enabled = true
cache_dir = ".litho/cache"
expire_hours = 8760  # 1 年

[knowledge]
[knowledge.local_docs]
enabled = true
watch_for_changes = true

[[knowledge.local_docs.categories]]
name = "architecture"
description = "架构决策记录和设计文档"
paths = ["docs/adr/*.md", "docs/design/*.md"]
target_agents = ["architecture_researcher"]

[[knowledge.local_docs.categories]]
name = "database"
description = "数据库模式和迁移脚本"
paths = ["db/**/*.sql", "migrations/**/*.sql"]
target_agents = ["database_analyzer"]
```

**最佳实践**：

- 将 litho.toml 配置文件存储在版本控制中，以便在团队成员之间共享设置
- 对敏感 API 密钥使用环境变量 (LITHO_LLM_API_KEY) 而不是硬编码在配置文件中
- 为 .litho/ 目录设置 .gitignore 条目，以避免提交缓存文件
- 配置特定文件扩展名和排除模式，以减少大型项目的分析时间
- 在 CI/CD 中谨慎使用 --force 标志，以避免不必要的 API 调用和处理时间

### LLM 提供程序配置

Deepwiki-rs 支持 8 个 LLM 提供程序，包括 OpenAI、Anthropic、Google Gemini、Moonshot、DeepSeek、Mistral、OpenRouter 和本地 Ollama 实例。配置支持双模型设置，可在高效模型和强大模型之间自动回退。

**示例代码**：

```toml
# 对于 OpenAI
[llm]
provider = "openai"
api_key = "sk-..."
model_efficient = "gpt-4o-mini"
model_powerful = "gpt-4o"

# 对于 Ollama（本地推理）
[llm]
provider = "ollama"
api_base_url = "http://localhost:11434"
model_efficient = "llama3.1:8b"
model_powerful = "llama3.1:70b"

# 对于 DeepSeek
[llm]
provider = "deepseek"
api_key = "${DEEPSEEK_API_KEY}"
api_base_url = "https://api.deepseek.com/v1"
model_efficient = "deepseek-chat"
model_powerful = "deepseek-reasoner"
```

**最佳实践**：

- 对于需要无外部互联网访问的气隙环境，使用 ModelScope 或 Ollama
- 配置双模型策略：高效模型用于例行分析，强大模型用于复杂架构推理
- 为速率受限的 API 设置适当的 retry_attempts 和 retry_delay_ms（推荐 5 次尝试，5 秒延迟）
- 如果希望完全手动控制智能体能力，则禁用预设工具 (disable_preset_tools = true)
- 在本地开发中使用 Ollama 以避免测试期间的 API 成本

### 知识库集成

集成外部知识源可实现 RAG（检索增强生成），用于上下文感知文档。支持按领域分类（架构、数据库、API）的 Markdown、SQL 和 PDF 文件，并对大型文档进行语义分块。

**示例代码**：

```toml
# 在 CI/CD 流水线中（GitHub Actions 示例）
- name: Cache Litho Knowledge Base
  uses: actions/cache@v3
  with:
    path: .litho/cache
    key: litho-${{ hashFiles('docs/**/*.md', 'db/**/*.sql') }}

- name: Sync External Knowledge
  run: litho sync-knowledge --force
  env:
    LITHO_LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

- name: Generate Documentation
  run: litho --config litho.ci.toml

# 领域特定知识的类别配置
[[knowledge.local_docs.categories]]
name = "api"
description = "外部 API 规范"
paths = ["api-specs/*.yaml", "openapi/*.json"]
target_agents = ["boundary_analyzer"]

[knowledge.local_docs.categories.chunking]
enabled = true
strategy = "semantic"
max_chunk_size = 8000
chunk_overlap = 200
```

**最佳实践**：

- 在 CI 中运行 litho sync-knowledge 在文档生成之前，以确保外部文档被索引
- 在 CI 运行之间缓存 .litho/ 目录以加速后续执行
- 仅在文档源更改时使用 --force 以避免不必要的处理
- 按类别组织知识库文档，使用特定的 target_agents 以实现最佳上下文注入
- 仅在开发环境中启用 watch_for_changes，不要在 CI 中启用

### CI/CD 流水线集成

Deepwiki-rs 设计用于 CI/CD 集成，以自动维护最新的架构文档。该工具以适当的错误代码退出以进行流水线故障检测，并支持非交互式执行。

**示例代码**：

```yaml
# GitLab CI 示例
generate-docs:
  stage: documentation
  image: rust:latest
  before_script:
    - cargo install deepwiki-rs
  script:
    # 如果外部文档更改则同步知识
    - litho sync-knowledge
    # 生成文档
    - litho --config litho.toml
  artifacts:
    paths:
      - litho.docs/
    expire_in: 1 month
  only:
    - main
    - tags

# CI 环境的高级配置
# litho.toml
include_tests = false
include_hidden = false
max_file_size = 65536  # CI 速度的 64KB 限制

[llm]
max_parallels = 5  # 在 CI 环境中增加并行度
timeout_seconds = 120  # CI 中更短的超时

# 排除 CI 生成的文件
excluded_files = ["*.lock", "*.log", ".env.ci", "*.tmp"]
```

**最佳实践**：

- 将文档生成添加为计划的 CI 作业（每周）而不是每次提交，以管理成本
- 将生成的文档提交到单独的分支或文档存储库
- 使用 include_tests = false 并排除构建产物以减少 token 消耗
- 设置 max_file_size 限制以防止处理大型生成的文件
- 仅为发布标签生成文档以保持稳定的文档版本
- 在 CI 流水线中使用 markdown 检查器验证生成的文档

---

**分析置信度**: 8.0/10
